# SPDX-License-Identifier: Apache-2.0
# based on: https://github.com/huggingface/tokenizers/tree/master/bindings/python

import json
from tokenizers import CharBPETokenizer, BertWordPieceTokenizer
from transformers import BertTokenizer

# Get all words from input vocabulary.
vocab_file = "/home/tkornuta/data/brain2/models/model_goal/enc_vocab.json"
with open(vocab_file) as f:
    vocab = json.load(f)
for k, v in vocab.items():
    print(k, ": ", v)

# Extend by tokens required by BERTWordPiece.
vocab["[UNK]"] = len(vocab)
vocab["[SEP]"] = len(vocab)
vocab["[CLS]"] = len(vocab)
vocab["[PAD]"] = len(vocab)
vocab["[MASK]"] = len(vocab)
# Add question mark?
#vocab["?"] = len(vocab)

# Initialize a tokenizer
#tokenizer = BertWordPieceTokenizer(vocab=vocab)
tokenizer = BertTokenizer.from_pretrained("bert-base-cased")

# Then train it!
#tokenizer.train([ "/home/tkornuta/data/brain2/sierra_5k_v1.csv" ], vocab_size=100)
#print("Vocabulary size: ", tokenizer.get_vocab_size())
print("Vocabulary size: ", tokenizer.vocab_size)
#for k, v in tokenizer.get_vocab().items():
#    print(k, ": ", v)

# Samples from 5k - human labels.
# data_00050000_00052798.gif,"Disjoint the given stacks to form a new stack with blue, red blocks.","Make a new stack with blue, red blocks."
# data_00150000_00150539.gif,Place all the blocks individually on the surface.,Disjoint the given stack of blocks.
# data_00110000_00110725.gif,"Separate the given stack to form yellow, red blocks stack.",Remove 2nd and 4th blocks from the given stack.
# data_00120000_00120478.gif,Remove 1st and 2nd block from the given stack and form stack with blue on top of yellow block.,Do not touch green and red block and form another stack with blue and yellow block

# Now, let's use it:
#input = "I can feel the magic, can you?"
input = "Disjoint the given stacks to form a new stack with blue, red blocks."
#input = "Make a new stack with blue, red blocks."
print(input)
encoded = tokenizer.encode(input)#, return_tensors="pt")
print(encoded)

# missing vocabulary! - > disjoint, blocks etc. -> not present in the vocabulary as it is created from pattern commands,
# not the ones generated by humans??

#print(encoded.ids)
#print(encoded.tokens)
#print(tokenizer.decode(encoded.ids))
print(tokenizer.decode(encoded))

# And finally save it somewhere
#tokenizer.save("./path/to/directory/my-bpe.tokenizer.json")